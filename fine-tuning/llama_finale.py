"""
Script 3 - Architecture et Configuration du Fine-tuning
Strat√©gie: LoRA (Low-Rank Adaptation)
Pourquoi LoRA? L√©ger, rapide, pr√©serve les connaissances du mod√®le
"""

import json
from pathlib import Path
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import numpy as np

# ==================== CONFIGURATION ====================

class FineTuningConfig:
    """Configuration compl√®te du fine-tuning"""
    
    def __init__(self):
        self.racine = Path(r"C:\Users\ZAID\OneDrive\Documents\3eme_gds\DL\DeepLearning_1")
        self.finetuning_dir = self.racine / "finetuning_data"
        
        # Datasets
        self.dataset_train = self.finetuning_dir / "02_dataset_train.jsonl"
        self.dataset_validation = self.finetuning_dir / "02_dataset_validation.jsonl"
        
        # Mod√®le de base
        self.model_name = "meta-llama/llama-4-scout-17b-16e-instruct"
        
        # Sortie
        self.output_dir = self.finetuning_dir / "llama_finetuned"
        self.checkpoints_dir = self.output_dir / "checkpoints"
        
        # LoRA Configuration
        self.lora_config = {
            "r": 16,                    # Rank (8, 16, 32) - Plus √©lev√© = plus de capacit√©
            "lora_alpha": 32,           # Alpha (g√©n√©ralement 2*r)
            "target_modules": [         # Quelles couches modifier
                "q_proj",               # Query projection
                "v_proj",               # Value projection
                "k_proj",               # Key projection
                "o_proj",               # Output projection
                "gate_proj",            # Gate projection (pour LLaMA)
                "up_proj",              # Up projection
                "down_proj"             # Down projection
            ],
            "lora_dropout": 0.05,       # Dropout pour r√©gularisation
            "bias": "none",             # Pas de biais adaptatif
            "task_type": "CAUSAL_LM"    # T√¢che de langage causal
        }
        
        # Training Hyperparameters
        self.training_args = {
            "output_dir": str(self.output_dir),
            "num_train_epochs": 3,              # Nombre d'√©poques
            "per_device_train_batch_size": 2,   # Batch size (ajuster selon GPU)
            "per_device_eval_batch_size": 2,
            "gradient_accumulation_steps": 4,   # Simule batch_size=8
            "learning_rate": 2e-4,              # Learning rate pour LoRA
            "warmup_steps": 100,                # Warmup
            "logging_steps": 10,                # Log tous les 10 steps
            "eval_steps": 50,                   # √âvaluer tous les 50 steps
            "save_steps": 100,                  # Sauvegarder tous les 100 steps
            "save_total_limit": 3,              # Garder 3 checkpoints max
            "evaluation_strategy": "steps",
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False,
            "fp16": True,                       # Mixed precision (si GPU compatible)
            "optim": "paged_adamw_8bit",        # Optimiseur efficace en m√©moire
            "report_to": "none"                 # Pas de logging externe
        }
        
        # Cr√©er les dossiers
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoints_dir.mkdir(parents=True, exist_ok=True)

# ==================== PR√âPARATION DES DONN√âES ====================

def prepare_dataset(tokenizer, data_file, max_length=512):
    """Pr√©pare le dataset pour le fine-tuning"""
    
    print(f"\nüìÇ Chargement: {data_file.name}")
    
    # Charger le dataset JSONL
    dataset = load_dataset('json', data_files=str(data_file), split='train')
    print(f"‚úÖ {len(dataset)} exemples charg√©s")
    
    def format_prompt(example):
        """Formate l'exemple en prompt conversationnel"""
        messages = example['messages']
        
        # Format pour Llama (style chat)
        formatted = ""
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == "system":
                formatted += f"<|system|>\n{content}\n"
            elif role == "user":
                formatted += f"<|user|>\n{content}\n"
            elif role == "assistant":
                formatted += f"<|assistant|>\n{content}\n"
        
        return {"text": formatted}
    
    # Formater tous les exemples
    dataset = dataset.map(format_prompt)
    
    def tokenize_function(examples):
        """Tokenize les exemples"""
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=max_length,
            padding="max_length"
        )
    
    # Tokenizer
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )
    
    return tokenized_dataset

# ==================== CHARGEMENT DU MOD√àLE ====================

def load_model_and_tokenizer(config):
    """Charge le mod√®le et le tokenizer"""
    
    print("\n" + "="*70)
    print("ü§ñ CHARGEMENT DU MOD√àLE")
    print("="*70)
    
    print(f"\nüì¶ Mod√®le: {config.model_name}")
    
    # Tokenizer
    print("üìù Chargement du tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        config.model_name,
        trust_remote_code=True
    )
    
    # Ajouter un padding token si absent
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("‚úÖ Tokenizer charg√©")
    
    # Mod√®le
    print(f"üß† Chargement du mod√®le (peut prendre quelques minutes)...")
    
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        torch_dtype=torch.float16,      # Utiliser float16 pour √©conomiser m√©moire
        device_map="auto",              # R√©partition automatique sur GPU
        trust_remote_code=True
    )
    
    print("‚úÖ Mod√®le de base charg√©")
    
    # Afficher les infos du mod√®le
    total_params = sum(p.numel() for p in model.parameters())
    print(f"üìä Param√®tres totaux: {total_params:,}")
    
    return model, tokenizer

# ==================== APPLICATION DE LoRA ====================

def apply_lora(model, config):
    """Applique LoRA au mod√®le"""
    
    print("\n" + "="*70)
    print("üîß APPLICATION DE LoRA")
    print("="*70)
    
    # Configuration LoRA
    lora_config = LoraConfig(
        r=config.lora_config["r"],
        lora_alpha=config.lora_config["lora_alpha"],
        target_modules=config.lora_config["target_modules"],
        lora_dropout=config.lora_config["lora_dropout"],
        bias=config.lora_config["bias"],
        task_type=TaskType.CAUSAL_LM
    )
    
    print(f"üìã Configuration LoRA:")
    print(f"   ‚Ä¢ Rank (r): {config.lora_config['r']}")
    print(f"   ‚Ä¢ Alpha: {config.lora_config['lora_alpha']}")
    print(f"   ‚Ä¢ Dropout: {config.lora_config['lora_dropout']}")
    print(f"   ‚Ä¢ Modules cibl√©s: {', '.join(config.lora_config['target_modules'][:3])}...")
    
    # Appliquer LoRA
    model = get_peft_model(model, lora_config)
    
    # Compter les param√®tres entra√Ænables
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    
    print(f"\nüìä R√©sultat:")
    print(f"   ‚Ä¢ Param√®tres entra√Ænables: {trainable_params:,}")
    print(f"   ‚Ä¢ Param√®tres totaux: {total_params:,}")
    print(f"   ‚Ä¢ Pourcentage entra√Ænable: {100 * trainable_params / total_params:.2f}%")
    print(f"   ‚Ä¢ üî• R√©duction: {100 * (1 - trainable_params / total_params):.1f}% de param√®tres gel√©s!")
    
    return model

# ==================== ENTRA√éNEMENT ====================

def train_model(model, tokenizer, train_dataset, eval_dataset, config):
    """Lance l'entra√Ænement du mod√®le"""
    
    print("\n" + "="*70)
    print("üöÄ LANCEMENT DE L'ENTRA√éNEMENT")
    print("="*70)
    
    # Arguments d'entra√Ænement
    training_args = TrainingArguments(**config.training_args)
    
    print(f"\nüìã Configuration:")
    print(f"   ‚Ä¢ √âpoques: {config.training_args['num_train_epochs']}")
    print(f"   ‚Ä¢ Batch size: {config.training_args['per_device_train_batch_size']}")
    print(f"   ‚Ä¢ Gradient accumulation: {config.training_args['gradient_accumulation_steps']}")
    print(f"   ‚Ä¢ Learning rate: {config.training_args['learning_rate']}")
    print(f"   ‚Ä¢ Exemples train: {len(train_dataset)}")
    print(f"   ‚Ä¢ Exemples validation: {len(eval_dataset)}")
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # Pas de Masked Language Modeling, juste Causal LM
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator
    )
    
    print(f"\nüèÉ Entra√Ænement en cours...")
    print("   (Cela peut prendre plusieurs heures selon votre mat√©riel)")
    
    # Lancer l'entra√Ænement
    trainer.train()
    
    print("\n‚úÖ Entra√Ænement termin√©!")
    
    # Sauvegarder le mod√®le final
    print(f"\nüíæ Sauvegarde du mod√®le final...")
    trainer.save_model()
    tokenizer.save_pretrained(config.output_dir)
    
    print(f"‚úÖ Mod√®le sauvegard√© dans: {config.output_dir}")
    
    return trainer

# ==================== MAIN ====================

def main():
    """Fonction principale"""
    
    print("\n" + "="*70)
    print("üéØ SCRIPT 3 - FINE-TUNING DE LLAMA AVEC LoRA")
    print("="*70)
    
    # Configuration
    config = FineTuningConfig()
    
    # V√©rifier la disponibilit√© du GPU
    if torch.cuda.is_available():
        print(f"\n‚úÖ GPU disponible: {torch.cuda.get_device_name(0)}")
        print(f"   M√©moire: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    else:
        print(f"\n‚ö†Ô∏è  Pas de GPU d√©tect√© - L'entra√Ænement sera TR√àS lent sur CPU")
        print("   Recommandation: Utiliser Google Colab ou un service cloud avec GPU")
    
    # V√©rifier les datasets
    if not config.dataset_train.exists():
        print(f"\n‚ùå Dataset train introuvable: {config.dataset_train}")
        print("   Ex√©cutez d'abord le Script 2 (pr√©paration dataset)")
        return
    
    if not config.dataset_validation.exists():
        print(f"\n‚ùå Dataset validation introuvable: {config.dataset_validation}")
        return
    
    try:
        # 1. Charger mod√®le et tokenizer
        model, tokenizer = load_model_and_tokenizer(config)
        
        # 2. Appliquer LoRA
        model = apply_lora(model, config)
        
        # 3. Pr√©parer les datasets
        print("\n" + "="*70)
        print("üìä PR√âPARATION DES DATASETS")
        print("="*70)
        
        train_dataset = prepare_dataset(tokenizer, config.dataset_train)
        eval_dataset = prepare_dataset(tokenizer, config.dataset_validation)
        
        # 4. Entra√Æner
        trainer = train_model(model, tokenizer, train_dataset, eval_dataset, config)
        
        # 5. Rapport final
        print("\n" + "="*70)
        print("üéâ FINE-TUNING TERMIN√â AVEC SUCC√àS!")
        print("="*70)
        
        print(f"""
üìÇ Mod√®le fine-tun√© sauvegard√© dans:
   {config.output_dir}

üìä Fichiers cr√©√©s:
   ‚Ä¢ adapter_config.json (configuration LoRA)
   ‚Ä¢ adapter_model.bin (poids LoRA)
   ‚Ä¢ tokenizer files

üéØ R√©sultats attendus:
   ‚úÖ 0% de r√©ponses non prouv√©es
   ‚úÖ Validation scientifique syst√©matique
   ‚úÖ Citations de sources
   ‚úÖ Maintien de l'empathie

üöÄ Prochaine √©tape:
   Script 4 - √âvaluation du mod√®le fine-tun√©
   ‚Üí Comparer avant/apr√®s
   ‚Üí V√©rifier les objectifs
        """)
        
    except Exception as e:
        print(f"\n‚ùå ERREUR pendant le fine-tuning:")
        print(f"   {str(e)}")
        print(f"\nüí° Solutions possibles:")
        print(f"   ‚Ä¢ V√©rifier que le mod√®le est accessible")
        print(f"   ‚Ä¢ R√©duire batch_size si erreur de m√©moire")
        print(f"   ‚Ä¢ Utiliser un GPU avec plus de m√©moire")

if __name__ == "__main__":
    main()